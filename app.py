import base64
import json
import logging
import os
import time

import streamlit as st
from dotenv import load_dotenv

from llm.V4 import Chatbot, AppBaseModel

load_dotenv()

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

client = Chatbot(api_key=os.getenv('OPENAI_API_KEY'), api_url=os.getenv('OPENAI_API_BASE'))


class StepResultModel(AppBaseModel):
    title: str
    content: str
    next_action: str
    confidence: float


def make_api_call(messages, max_tokens, temperature=0.5, is_final_answer=False, model="gpt-4o"):
    for attempt in range(3):
        try:
            logger.info(f"å°è¯•è¿›è¡ŒAPIè°ƒç”¨ (ç¬¬ {attempt + 1}/3 æ¬¡å°è¯•)")
            content, _, _, _ = client.ask(
                model=model,
                prompt=messages,
                json_format=True,
                max_tokens=max_tokens,
                temperature=temperature,
                response_model=StepResultModel
            )
            logger.info("APIè°ƒç”¨æˆåŠŸ")
            logger.info(content)
            return json.loads(content)
        except Exception as e:
            logger.error(f"APIè°ƒç”¨å¤±è´¥ (ç¬¬ {attempt + 1}/3 æ¬¡å°è¯•)ã€‚é”™è¯¯: {str(e)}")
            if attempt == 2:
                if is_final_answer:
                    logger.error("3æ¬¡å°è¯•åæœªèƒ½ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
                    return {
                        "title": "é”™è¯¯",
                        "content": f"3æ¬¡å°è¯•åæœªèƒ½ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚é”™è¯¯: {str(e)}",
                    }
                else:
                    logger.error("3æ¬¡å°è¯•åæœªèƒ½ç”Ÿæˆæ­¥éª¤")
                    return {
                        "title": "é”™è¯¯",
                        "content": f"3æ¬¡å°è¯•åæœªèƒ½ç”Ÿæˆæ­¥éª¤ã€‚é”™è¯¯: {str(e)}",
                        "next_action": "final_answer",
                    }
            logger.info("ç­‰å¾…1ç§’åé‡è¯•")
            time.sleep(1)  # é‡è¯•å‰ç­‰å¾…1ç§’


def generate_response(prompt, max_steps=5, temperature=0.5, model="gpt-4o"):
    logger.info(f"æ­£åœ¨ä¸ºæç¤ºç”Ÿæˆå›ç­”: {prompt}")
    messages = [
        {
            "role": "system",
            "content": """You are an AI assistant that explains your reasoning step by step, incorporating dynamic Chain of Thought (CoT), reflection, and verbal reinforcement learning. Follow these instructions:

1. Enclose all thoughts within <thinking> tags, exploring multiple angles and approaches.
2. Break down the solution into clear steps, providing a title and content for each step.
3. After each step, decide if you need another step or if you're ready to give the final answer.
4. Continuously adjust your reasoning based on intermediate results and reflections, adapting your strategy as you progress.
5. Regularly evaluate your progress, being critical and honest about your reasoning process.
6. Assign a quality score between 0.0 and 1.0 to guide your approach:
   - 0.8+: Continue current approach
   - 0.5-0.7: Consider minor adjustments
   - Below 0.5: Seriously consider backtracking and trying a different approach
7. If unsure or if your score is low, backtrack and try a different approach, explaining your decision.
8. For mathematical problems, show all work explicitly using LaTeX for formal notation and provide detailed proofs.
9. Explore multiple solutions individually if possible, comparing approaches in your reflections.
10. Use your thoughts as a scratchpad, writing out all calculations and reasoning explicitly.
11. Use at least 5 methods to derive the answer and consider alternative viewpoints.
12. Be aware of your limitations as an AI and what you can and cannot do.

After every 3 steps, perform a detailed self-reflection on your reasoning so far, considering potential biases and alternative viewpoints.

Respond in JSON format with 'title', 'content', 'next_action' (either 'continue', 'reflect', or 'final_answer'), and 'confidence' (a number between 0 and 1) keys.

Example of a valid JSON response:
```json
{
    "title": "Identifying Key Information",
    "content": "To begin solving this problem, we need to carefully examine the given information and identify the crucial elements that will guide our solution process. This involves...",
    "next_action": "continue",
    "confidence": 0.8
}```

Your goal is to demonstrate a thorough, adaptive, and self-reflective problem-solving process, emphasizing dynamic thinking and learning from your own reasoning.""",
        },
        {"role": "user", "content": prompt}
    ]

    steps = []
    step_count = 1
    total_thinking_time = 0

    while True:
        logger.info(f"å¼€å§‹ç¬¬ {step_count} æ­¥")
        start_time = time.time()
        step_data = make_api_call(messages, 4096, temperature=temperature, model=model)
        end_time = time.time()
        thinking_time = end_time - start_time
        total_thinking_time += thinking_time

        logger.info(f"ç¬¬ {step_count} æ­¥å®Œæˆã€‚æ€è€ƒæ—¶é—´: {thinking_time:.2f} ç§’")
        steps.append((f"{step_data['title']}", step_data["content"], thinking_time))

        messages.append({"role": "assistant", "content": json.dumps(step_data)})

        if step_data["next_action"] == "final_answer" and step_count < max_steps:
            messages.append({"role": "user",
                             "content": "Please continue your analysis with at least 5 more steps before providing the final answer."})
        elif step_data["next_action"] == "final_answer":
            logger.info("å·²è¾¾åˆ°æœ€ç»ˆç­”æ¡ˆæˆ–æœ€å¤§æ­¥éª¤æ•°")
            break
        elif step_data["next_action"] == 'reflect' or step_count % 3 == 0:
            messages.append({"role": "user",
                             "content": "Please perform a detailed self-reflection on your reasoning so far, considering potential biases and alternative viewpoints."})
        else:
            messages.append({"role": "user", "content": "Please continue with the next step in your analysis."})
        step_count += 1

        yield steps, None, None  # æˆ‘ä»¬ç°åœ¨yieldä¸‰ä¸ªå€¼,ä½†åªæœ‰stepsæ˜¯æœ‰æ„ä¹‰çš„

    # ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    messages.append({"role": "user",
                     "content": "Please provide a comprehensive final answer based on your reasoning above, summarizing key points and addressing any uncertainties. USE JSON Formate"})

    start_time = time.time()
    final_data = make_api_call(messages, 4096, temperature=temperature, is_final_answer=True, model=model)
    end_time = time.time()
    thinking_time = end_time - start_time
    total_thinking_time += thinking_time

    if 'content' in final_data:
        final_content = final_data["content"]
    elif 'final_answer' in final_data:
        final_content = final_data["final_answer"]
    else:
        final_content = json.dumps(final_data)
    logger.info(f"æœ€ç»ˆç­”æ¡ˆå·²ç”Ÿæˆã€‚æ€è€ƒæ—¶é—´: {thinking_time:.2f} ç§’")
    steps.append(("æœ€ç»ˆç­”æ¡ˆ", final_content, thinking_time))

    logger.info(f"æ€»æ€è€ƒæ—¶é—´: {total_thinking_time:.2f} ç§’")
    full_response = {"steps": steps, "total_thinking_time": total_thinking_time}
    yield steps, total_thinking_time, full_response


def get_binary_file_downloader_html(bin_file, file_label="æ–‡ä»¶"):
    with open(bin_file, "rb") as f:
        data = f.read()
    bin_str = base64.b64encode(data).decode()
    href = f"""
    <a href="data:application/octet-stream;base64,{bin_str}" download="{os.path.basename(bin_file)}"
       style="display: inline-block; padding: 0.5em 1em; color: white; background-color: #4CAF50; text-decoration: none; border-radius: 4px;">
        ğŸ“¥ ä¸‹è½½ {file_label}
    </a>
    """
    return href


def main():
    st.set_page_config(page_title="g1 åŸå‹", page_icon="ğŸ§ ", layout="wide")

    st.title("g1: ä½¿ç”¨ LLM åˆ›å»ºç±»ä¼¼ o1 çš„æ¨ç†é“¾")

    st.markdown(
        """
    <style>
        /* æ–°æ ·å¼ */
        h1, h2, h3 {
            color: #1e3a8a;
        }

        /* ä¾§è¾¹æ æ ·å¼è°ƒæ•´ */
        .css-1d391kg {
            padding-top: 1rem;
            padding-right: 0.5rem;
            padding-left: 0.5rem;
        }
        .css-1d391kg .block-container {
            padding-top: 1rem;
        }
        /* è°ƒæ•´ä¾§è¾¹æ å®½åº¦ */
        .css-1q1n0ol {
            max-width: 14rem;
        }
    </style>
    """,
        unsafe_allow_html=True,
    )

    st.markdown(
        """
    è¿™æ˜¯ä¸€ä¸ªæ—©æœŸåŸå‹,ä½¿ç”¨æç¤ºæ¥åˆ›å»ºç±»ä¼¼ o1 çš„æ¨ç†é“¾ä»¥æé«˜è¾“å‡ºå‡†ç¡®æ€§ã€‚å®ƒå¹¶ä¸å®Œç¾,å‡†ç¡®æ€§å°šæœªç»è¿‡æ­£å¼è¯„ä¼°ã€‚

    å¼€æº[ä»£ç åº“åœ¨æ­¤](https://github.com/Theigrams/g1)
    """
    )

    with st.sidebar:
        st.markdown("## ğŸ› ï¸ è®¾ç½®")

        st.markdown("<br>", unsafe_allow_html=True)  # æ·»åŠ é—´è·

        st.markdown("### ğŸ¤– æ¨¡å‹è®¾ç½®")
        model_options = [
            "claude-3-5-sonnet-20240620", "claude-3-haiku-20240307",
            "gpt-4o", "gpt-4o-mini", "gpt-3.5-turbo",
            "gemini-1.5-pro", "gemini-1.5-flash", "gemini-1.5-flash-8b-exp-0924",
            "qwen2-72b-instruct", "qwen2.5-72b-instruct",
            "llama-3.1-70b-versatile"
        ]
        selected_model = st.selectbox("é€‰æ‹©æ¨¡å‹", model_options)
        model = selected_model

        st.markdown("<br>", unsafe_allow_html=True)  # æ·»åŠ é—´è·

        st.markdown("### âš™ï¸ ç”Ÿæˆè®¾ç½®")
        max_steps = st.slider("æœ€å¤§æ­¥éª¤æ•°", 3, 32, 10)
        temperature = st.slider("æ¸©åº¦", 0.0, 1.0, 0.2, 0.1)

    # ç”¨æˆ·æŸ¥è¯¢çš„æ–‡æœ¬è¾“å…¥å’Œå‘é€æŒ‰é’®
    st.markdown("### ğŸ” è¾“å…¥æ‚¨çš„æŸ¥è¯¢")
    col1, col2 = st.columns([5, 1])  # åˆ›å»ºä¸¤åˆ—ï¼Œæ¯”ä¾‹ä¸º 5:1
    with col1:
        user_query = st.text_input("", placeholder="ä¾‹å¦‚ï¼š1.11 å’Œ 1.3 å“ªä¸ªå¤§?",
                                   label_visibility="collapsed")
    with col2:
        send_button = st.button("å‘é€")

    if send_button and user_query:
        with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):  # æ·»åŠ åŠ è½½æŒ‡ç¤ºå™¨
            # åˆ›å»ºç©ºå…ƒç´ ä»¥ä¿å­˜ç”Ÿæˆçš„æ–‡æœ¬å’Œæ€»æ—¶é—´
            response_container = st.empty()
            time_container = st.empty()
            download_container = st.empty()

            # ç”Ÿæˆå¹¶æ˜¾ç¤ºå›ç­”
            for steps, total_thinking_time, full_response in generate_response(
                    user_query, max_steps=max_steps, temperature=temperature, model=model
            ):
                with response_container.container():
                    for i, (title, content, thinking_time) in enumerate(steps):
                        if title.startswith("æœ€ç»ˆç­”æ¡ˆ"):
                            st.markdown(f"### ğŸ¯ {title}")
                            st.info(content)
                        else:
                            with st.expander(f"ğŸ§  {title} (æ€è€ƒæ—¶é—´: {thinking_time:.2f} ç§’)", expanded=True):
                                st.write(content)  # ä½¿ç”¨ write è€Œä¸æ˜¯ markdown ä»¥é¿å… HTML è½¬ä¹‰é—®é¢˜

                # ä»…åœ¨ç»“æŸæ—¶æ˜¾ç¤ºæ€»æ—¶é—´
                if total_thinking_time is not None and full_response is not None:
                    time_container.markdown(f"â±ï¸ **æ€»æ€è€ƒæ—¶é—´: {total_thinking_time:.2f} ç§’**")

                    # åˆ›å»º JSON æ–‡ä»¶å¹¶æä¾›ä¸‹è½½é“¾æ¥
                    json_filename = "reasoning_chain.json"
                    with open(json_filename, "w") as f:
                        json.dump(full_response, f, indent=2)

                    download_link = get_binary_file_downloader_html(json_filename, "å®Œæ•´æ¨ç†é“¾ JSON")
                    download_container.markdown(download_link, unsafe_allow_html=True)


if __name__ == "__main__":
    main()
