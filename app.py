import base64
import json
import logging
import os
import time

import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), base_url=os.getenv('OPENAI_API_BASE'))


def extract_first_json(response):
    json_start = response.index("{")
    json_end = response.find("}")
    return json.loads(response[json_start:json_end + 1])


def extract_json(response):
    response = response.replace("JSON\n", "").replace("json\n", "").replace("```", "")
    json_start = response.index("{")
    json_end = response.rfind("}")
    return json.loads(response[json_start:json_end + 1])


def make_api_call(messages, max_tokens, temperature=0.5, is_final_answer=False, model="gpt-4o"):
    for attempt in range(3):
        try:
            logger.info(f"å°è¯•è¿›è¡ŒAPIè°ƒç”¨ (ç¬¬ {attempt + 1}/3 æ¬¡å°è¯•)")
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                response_format={"type": "json_object"},
            )
            logger.info("APIè°ƒç”¨æˆåŠŸ")
            content = response.choices[0].message.content
            logger.info(content)
            return extract_first_json(content)
        except Exception as e:
            logger.error(f"APIè°ƒç”¨å¤±è´¥ (ç¬¬ {attempt + 1}/3 æ¬¡å°è¯•)ã€‚é”™è¯¯: {str(e)}")
            if attempt == 2:
                if is_final_answer:
                    logger.error("3æ¬¡å°è¯•åæœªèƒ½ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
                    return {
                        "title": "é”™è¯¯",
                        "content": f"3æ¬¡å°è¯•åæœªèƒ½ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚é”™è¯¯: {str(e)}",
                    }
                else:
                    logger.error("3æ¬¡å°è¯•åæœªèƒ½ç”Ÿæˆæ­¥éª¤")
                    return {
                        "title": "é”™è¯¯",
                        "content": f"3æ¬¡å°è¯•åæœªèƒ½ç”Ÿæˆæ­¥éª¤ã€‚é”™è¯¯: {str(e)}",
                        "next_action": "final_answer",
                    }
            logger.info("ç­‰å¾…1ç§’åé‡è¯•")
            time.sleep(1)  # é‡è¯•å‰ç­‰å¾…1ç§’


def generate_response(prompt, max_steps=5, temperature=0.5, model="gpt-4o"):
    logger.info(f"æ­£åœ¨ä¸ºæç¤ºç”Ÿæˆå›ç­”: {prompt}")
    messages = [
        {
            "role": "system",
            "content": """ä½ æ˜¯ä¸€ä½ä¸“å®¶çº§AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿé€æ­¥è§£é‡Šä½ çš„æ¨ç†è¿‡ç¨‹ã€‚
å¯¹äºæ¯ä¸€æ­¥ï¼Œè¯·æä¾›ä¸€ä¸ªæè¿°è¯¥æ­¥éª¤å†…å®¹çš„æ ‡é¢˜ï¼Œä»¥åŠå…·ä½“å†…å®¹ã€‚
å†³å®šæ˜¯å¦éœ€è¦å¦ä¸€ä¸ªæ­¥éª¤æˆ–æ˜¯å¦å‡†å¤‡å¥½ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚ä»¥ JSON æ ¼å¼å›åº”ï¼ŒåŒ…å« 'title'ã€'content' å’Œ 'next_action' (next_action çš„å€¼åªæœ‰ 'continue' æˆ– 'final_answer') ã€‚
å°½å¯èƒ½ä½¿ç”¨å¤šä¸ªæ¨ç†æ­¥éª¤ï¼Œè‡³å°‘ 3 ä¸ªã€‚
è¯·æ³¨æ„ä½ ä½œä¸ºè¯­è¨€æ¨¡å‹çš„å±€é™æ€§ï¼Œä»¥åŠä½ èƒ½åšå’Œä¸èƒ½åšçš„äº‹æƒ…ã€‚åœ¨ä½ çš„æ¨ç†ä¸­ï¼ŒåŒ…æ‹¬å¯¹æ›¿ä»£ç­”æ¡ˆçš„æ¢ç´¢ã€‚è€ƒè™‘åˆ°ä½ å¯èƒ½æ˜¯é”™çš„ï¼Œå¦‚æœä½ çš„æ¨ç†ä¸­æœ‰é”™è¯¯ï¼Œé”™è¯¯å¯èƒ½åœ¨å“ªé‡Œã€‚
å……åˆ†æµ‹è¯•æ‰€æœ‰å…¶ä»–å¯èƒ½æ€§ã€‚ä½ å¯èƒ½ä¼šçŠ¯é”™ã€‚å½“ä½ è¯´ä½ åœ¨é‡æ–°å®¡è§†æ—¶ï¼Œè¯·çœŸæ­£åœ°é‡æ–°å®¡è§†ï¼Œå¹¶ä½¿ç”¨å¦ä¸€ç§æ–¹æ³•æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚ä¸è¦åªæ˜¯è¯´ä½ åœ¨é‡æ–°å®¡è§†ã€‚
ä½¿ç”¨è‡³å°‘ 3 ç§æ–¹æ³•æ¥å¾—å‡ºç­”æ¡ˆã€‚ä½¿ç”¨æœ€ä½³å®è·µã€‚

æœ‰æ•ˆJSONå“åº”çš„ç¤ºä¾‹:
```json
{
    "title": "è¯†åˆ«å…³é”®ä¿¡æ¯",
    "content": "ä¸ºäº†å¼€å§‹è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦ä»”ç»†æ£€æŸ¥ç»™å®šçš„ä¿¡æ¯ï¼Œå¹¶è¯†åˆ«å‡ºå°†æŒ‡å¯¼æˆ‘ä»¬è§£å†³è¿‡ç¨‹çš„å…³é”®å…ƒç´ ã€‚è¿™æ¶‰åŠåˆ°...",
    "next_action": "continue"
}```

æ³¨æ„ï¼Œæ¯æ¬¡åªè¾“å‡ºä¸€ä¸ªæ¨ç†æ­¥éª¤ï¼Œä¸è¦å…¨éƒ¨è¾“å‡ºï¼Œè¯·ç­‰å¾…ä¸‹ä¸€æ­¥çš„æŒ‡ä»¤å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå†ç»§ç»­è¾“å‡ºæ¨ç†æ­¥éª¤ã€‚
""",
        },
        {"role": "user", "content": prompt},
        {
            "role": "assistant",
            "content": "è°¢è°¢ï¼æˆ‘ç°åœ¨å°†æŒ‰ç…§æˆ‘çš„æŒ‡ç¤ºï¼Œä»åˆ†è§£é—®é¢˜å¼€å§‹ï¼Œé€æ­¥æ€è€ƒã€‚",
        },
        {"role": "user", "content": "continue"}
    ]

    steps = []
    step_count = 1
    total_thinking_time = 0

    while True:
        logger.info(f"å¼€å§‹ç¬¬ {step_count} æ­¥")
        start_time = time.time()
        step_data = make_api_call(messages, 4096, temperature=temperature, model=model)
        end_time = time.time()
        thinking_time = end_time - start_time
        total_thinking_time += thinking_time

        logger.info(f"ç¬¬ {step_count} æ­¥å®Œæˆã€‚æ€è€ƒæ—¶é—´: {thinking_time:.2f} ç§’")
        steps.append((f"æ­¥éª¤ {step_count}: {step_data['title']}", step_data["content"], thinking_time))

        messages.append({"role": "assistant", "content": json.dumps(step_data)})

        if step_data["next_action"] == "final_answer" or step_count >= max_steps:
            logger.info("å·²è¾¾åˆ°æœ€ç»ˆç­”æ¡ˆæˆ–æœ€å¤§æ­¥éª¤æ•°")
            break
        messages.append({"role": "user", "content": step_data['next_action']})
        step_count += 1

        yield steps, None, None  # æˆ‘ä»¬ç°åœ¨yieldä¸‰ä¸ªå€¼,ä½†åªæœ‰stepsæ˜¯æœ‰æ„ä¹‰çš„

    # ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    messages.append({"role": "user", "content": "è¯·æ ¹æ®ä½ ä¸Šé¢çš„æ¨ç†ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚æ³¨æ„è¿˜è¦ä»¥ JSON çš„æ ¼å¼è¾“å‡º"})

    start_time = time.time()
    final_data = make_api_call(messages, 4096, temperature=temperature, is_final_answer=True, model=model)
    end_time = time.time()
    thinking_time = end_time - start_time
    total_thinking_time += thinking_time

    logger.info(f"æœ€ç»ˆç­”æ¡ˆå·²ç”Ÿæˆã€‚æ€è€ƒæ—¶é—´: {thinking_time:.2f} ç§’")
    steps.append(("æœ€ç»ˆç­”æ¡ˆ", final_data["content"], thinking_time))

    logger.info(f"æ€»æ€è€ƒæ—¶é—´: {total_thinking_time:.2f} ç§’")
    full_response = {"steps": steps, "total_thinking_time": total_thinking_time}
    yield steps, total_thinking_time, full_response


def get_binary_file_downloader_html(bin_file, file_label="æ–‡ä»¶"):
    with open(bin_file, "rb") as f:
        data = f.read()
    bin_str = base64.b64encode(data).decode()
    href = f"""
    <a href="data:application/octet-stream;base64,{bin_str}" download="{os.path.basename(bin_file)}"
       style="display: inline-block; padding: 0.5em 1em; color: white; background-color: #4CAF50; text-decoration: none; border-radius: 4px;">
        ğŸ“¥ ä¸‹è½½ {file_label}
    </a>
    """
    return href


def main():
    st.set_page_config(page_title="g1 åŸå‹", page_icon="ğŸ§ ", layout="wide")

    st.title("g1: ä½¿ç”¨ GPT-4o åˆ›å»ºç±»ä¼¼ o1 çš„æ¨ç†é“¾")

    st.markdown(
        """
    <style>
        /* æ–°æ ·å¼ */
        h1, h2, h3 {
            color: #1e3a8a;
        }

        /* ä¾§è¾¹æ æ ·å¼è°ƒæ•´ */
        .css-1d391kg {
            padding-top: 1rem;
            padding-right: 0.5rem;
            padding-left: 0.5rem;
        }
        .css-1d391kg .block-container {
            padding-top: 1rem;
        }
        /* è°ƒæ•´ä¾§è¾¹æ å®½åº¦ */
        .css-1q1n0ol {
            max-width: 14rem;
        }
    </style>
    """,
        unsafe_allow_html=True,
    )

    st.markdown(
        """
    è¿™æ˜¯ä¸€ä¸ªæ—©æœŸåŸå‹,ä½¿ç”¨æç¤ºæ¥åˆ›å»ºç±»ä¼¼ o1 çš„æ¨ç†é“¾ä»¥æé«˜è¾“å‡ºå‡†ç¡®æ€§ã€‚å®ƒå¹¶ä¸å®Œç¾,å‡†ç¡®æ€§å°šæœªç»è¿‡æ­£å¼è¯„ä¼°ã€‚

    å¼€æº[ä»£ç åº“åœ¨æ­¤](https://github.com/Theigrams/g1)
    """
    )

    with st.sidebar:
        st.markdown("## ğŸ› ï¸ è®¾ç½®")

        st.markdown("<br>", unsafe_allow_html=True)  # æ·»åŠ é—´è·

        st.markdown("### ğŸ¤– æ¨¡å‹è®¾ç½®")
        model_options = [
            "gpt-4o", "gpt-4o-mini", "gpt-3.5-turbo",
            "gemini-1.5-pro", "gemini-1.5-flash", "gemini-1.5-flash-8b-exp-0924",
            "claude-3-5-sonnet-20240620", "claude-3-haiku-20240307",
            "qwen2-72b-instruct", "qwen2.5-72b-instruct",
            "llama-3.1-70b-versatile"
        ]
        selected_model = st.selectbox("é€‰æ‹©æ¨¡å‹", model_options)
        model = selected_model

        st.markdown("<br>", unsafe_allow_html=True)  # æ·»åŠ é—´è·

        st.markdown("### âš™ï¸ ç”Ÿæˆè®¾ç½®")
        max_steps = st.slider("æœ€å¤§æ­¥éª¤æ•°", 3, 32, 10)
        temperature = st.slider("æ¸©åº¦", 0.0, 1.0, 0.5, 0.1)

    # ç”¨æˆ·æŸ¥è¯¢çš„æ–‡æœ¬è¾“å…¥å’Œå‘é€æŒ‰é’®
    st.markdown("### ğŸ” è¾“å…¥æ‚¨çš„æŸ¥è¯¢")
    col1, col2 = st.columns([5, 1])  # åˆ›å»ºä¸¤åˆ—ï¼Œæ¯”ä¾‹ä¸º 5:1
    with col1:
        user_query = st.text_input("", placeholder="ä¾‹å¦‚ï¼šå•è¯'strawberry'ä¸­æœ‰å¤šå°‘ä¸ª'R'?",
                                   label_visibility="collapsed")
    with col2:
        send_button = st.button("å‘é€")

    if send_button and user_query:
        with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):  # æ·»åŠ åŠ è½½æŒ‡ç¤ºå™¨
            # åˆ›å»ºç©ºå…ƒç´ ä»¥ä¿å­˜ç”Ÿæˆçš„æ–‡æœ¬å’Œæ€»æ—¶é—´
            response_container = st.empty()
            time_container = st.empty()
            download_container = st.empty()

            # ç”Ÿæˆå¹¶æ˜¾ç¤ºå›ç­”
            for steps, total_thinking_time, full_response in generate_response(
                    user_query, max_steps=max_steps, temperature=temperature, model=model
            ):
                with response_container.container():
                    for i, (title, content, thinking_time) in enumerate(steps):
                        if title.startswith("æœ€ç»ˆç­”æ¡ˆ"):
                            st.markdown(f"### ğŸ¯ {title}")
                            st.info(content)
                        else:
                            with st.expander(f"ğŸ§  {title} (æ€è€ƒæ—¶é—´: {thinking_time:.2f} ç§’)", expanded=True):
                                st.write(content)  # ä½¿ç”¨ write è€Œä¸æ˜¯ markdown ä»¥é¿å… HTML è½¬ä¹‰é—®é¢˜

                # ä»…åœ¨ç»“æŸæ—¶æ˜¾ç¤ºæ€»æ—¶é—´
                if total_thinking_time is not None and full_response is not None:
                    time_container.markdown(f"â±ï¸ **æ€»æ€è€ƒæ—¶é—´: {total_thinking_time:.2f} ç§’**")

                    # åˆ›å»º JSON æ–‡ä»¶å¹¶æä¾›ä¸‹è½½é“¾æ¥
                    json_filename = "reasoning_chain.json"
                    with open(json_filename, "w") as f:
                        json.dump(full_response, f, indent=2)

                    download_link = get_binary_file_downloader_html(json_filename, "å®Œæ•´æ¨ç†é“¾ JSON")
                    download_container.markdown(download_link, unsafe_allow_html=True)


if __name__ == "__main__":
    main()
